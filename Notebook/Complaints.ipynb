{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf630b-ba5f-4bfe-8ca6-95f3915330ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dff9ccf-e676-4f0b-b6a5-5650fb44af8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\thori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\thori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\thori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total complaints processed: 903\n",
      "\n",
      "LDA Topics (Bag-of-Words):\n",
      "Topic 1: credit, forbearance, reporting, reported, information, disaster, loss, loan, mitigation, bureau\n",
      "Topic 2: shellpoint, mortgage, servicing, loan, payment, foreclosure, complaint, year, error, account\n",
      "Topic 3: payment, mortgage, loan, account, year, shellpoint, would, time, told, month\n",
      "Topic 4: insurance, escrow, newrez, mortgage, property, policy, document, loan, shellpoint, company\n",
      "Topic 5: servicing, payment, newrez, cfpb, failure, violation, usc, borrower, unapplied, ledger\n",
      "\n",
      "LDA Topics (TF-IDF):\n",
      "Topic 1: overage, referring, word, pertaining, delivered, joint, belong, miscommunication, maintenance, recalculated\n",
      "Topic 2: overage, referring, word, eadacpa, pertaining, delivered, joint, belong, miscommunication, maintenance\n",
      "Topic 3: payment, shellpoint, mortgage, loan, escrow, year, account, newrez, insurance, company\n",
      "Topic 4: overage, referring, word, pertaining, delivered, joint, belong, miscommunication, maintenance, recalculated\n",
      "Topic 5: overage, referring, word, pertaining, delivered, joint, belong, miscommunication, maintenance, recalculated\n",
      "\n",
      "NMF Topics (TF-IDF):\n",
      "Topic 1: told, called, loan, would, mortgage, company, call, get, time, year\n",
      "Topic 2: shellpoint, foreclosure, servicing, loan, cfpb, violation, complaint, respa, modification, response\n",
      "Topic 3: escrow, insurance, taes, ta, account, policy, newrez, property, paid, analysis\n",
      "Topic 4: payment, late, fee, made, principal, account, month, applied, balance, amount\n",
      "Topic 5: credit, reporting, reported, information, inaccurate, bureau, fcra, account, report, dispute\n",
      "\n",
      "NMF Topics (BoW):\n",
      "Topic 1: servicing, newrez, cfpb, usc, violation, failure, foreclosure, mortgage, borrower, consumer\n",
      "Topic 2: payment, made, account, fee, unapplied, im, balance, late, pay, see\n",
      "Topic 3: shellpoint, payment, modification, foreclosure, application, error, servicing, response, loss, mitigation\n",
      "Topic 4: mortgage, loan, year, time, document, told, would, received, email, company\n",
      "Topic 5: escrow, insurance, payment, account, policy, error, ta, taes, shortage, notice\n",
      "\n",
      "All results have been saved to Excel files.\n",
      "\n",
      "Topic prevalence comparison (all models):\n",
      "\n",
      "LDA_BoW_Label Topic Prevalence:\n",
      "Topic 3: Payments & Accounts: 445 complaints\n",
      "Topic 2: Mortgage Servicing: 258 complaints\n",
      "Topic 4: Insurance & Escrow: 107 complaints\n",
      "Topic 5: Servicing Errors: 51 complaints\n",
      "Topic 1: Credit & Reporting: 42 complaints\n",
      "\n",
      "LDA_TFIDF_Label Topic Prevalence:\n",
      "Topic 3: Insurance & Escrow: 903 complaints\n",
      "\n",
      "NMF_TFIDF_Label Topic Prevalence:\n",
      "Topic 2: Mortgage Foreclosure: 253 complaints\n",
      "Topic 1: Communication & Loan Info: 233 complaints\n",
      "Topic 4: Payments & Fees: 176 complaints\n",
      "Topic 3: Insurance & Property: 161 complaints\n",
      "Topic 5: Credit Reporting: 80 complaints\n",
      "\n",
      "NMF_BoW_Label Topic Prevalence:\n",
      "Topic 4: Insurance & Escrow: 381 complaints\n",
      "Topic 3: Payment Issues: 170 complaints\n",
      "Topic 2: Mortgage & Servicing: 169 complaints\n",
      "Topic 5: Servicing Errors: 116 complaints\n",
      "Topic 1: Credit Reporting: 67 complaints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk_data_path = r'C:\\Users\\thori\\AppData\\Roaming\\nltk_data'\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n",
    "\n",
    "# Load the data\n",
    "file_path = r'complaints.xlsx.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "complaint_column = 'Consumer complaint narrative'  \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove redacted values (XXXX)\n",
    "        text = re.sub(r'x+', '', text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        \n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply cleaning to the narrative column\n",
    "df['cleaned_text'] = df[complaint_column].apply(clean_text)\n",
    "\n",
    "# Save cleaned data to a new Excel file\n",
    "df.to_excel('cleaned_complaints.xlsx', index=False)\n",
    "\n",
    "# Create document-term matrices\n",
    "# Bag-of-Words (CountVectorizer)\n",
    "count_vectorizer = CountVectorizer(min_df=5, max_df=0.9)\n",
    "count_matrix = count_vectorizer.fit_transform(df['cleaned_text'])\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized matrices to CSV\n",
    "pd.DataFrame(count_matrix.toarray(), columns=count_feature_names).to_csv('bow_matrix.csv')\n",
    "pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names).to_csv('tfidf_matrix.csv')\n",
    "\n",
    "# Topic Modeling\n",
    "# Number of topics - adjust based on your needs\n",
    "n_topics = 5\n",
    "\n",
    "# LDA with CountVectorizer\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_output = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "# LDA with TF-IDF\n",
    "lda_tfidf_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_tfidf_output = lda_tfidf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# NMF with CountVectorizer\n",
    "nmf_bow_model = NMF(n_components=n_topics, random_state=42)\n",
    "nmf_bow_output = nmf_bow_model.fit_transform(count_matrix)\n",
    "\n",
    "# NMF with TF-IDF\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "nmf_output = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Function to display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append({f\"Topic {topic_idx+1}\": top_words})\n",
    "    return topics\n",
    "\n",
    "# Get top words for each topic\n",
    "lda_topics = display_topics(lda_model, count_feature_names)\n",
    "nmf_topics = display_topics(nmf_model, tfidf_feature_names)\n",
    "lda_tfidf_topics = display_topics(lda_tfidf_model, tfidf_feature_names)\n",
    "nmf_bow_topics = display_topics(nmf_bow_model, count_feature_names)\n",
    "\n",
    "# Create DataFrames for topics\n",
    "lda_topics_df = pd.DataFrame()\n",
    "nmf_topics_df = pd.DataFrame()\n",
    "lda_tfidf_df = pd.DataFrame()\n",
    "nmf_bow_df = pd.DataFrame()\n",
    "for topic_dict in lda_topics:\n",
    "    for topic_name, words in topic_dict.items():\n",
    "        lda_topics_df[topic_name] = pd.Series(words)\n",
    "\n",
    "for topic_dict in nmf_topics:\n",
    "    for topic_name, words in topic_dict.items():\n",
    "        nmf_topics_df[topic_name] = pd.Series(words)\n",
    "\n",
    "for topic_dict in lda_tfidf_topics:\n",
    "    for topic_name, words in topic_dict.items():\n",
    "        lda_tfidf_df[topic_name] = pd.Series(words)\n",
    "\n",
    "for topic_dict in nmf_bow_topics:\n",
    "    for topic_name, words in topic_dict.items():\n",
    "        nmf_bow_df[topic_name] = pd.Series(words)\n",
    "\n",
    "\n",
    "# Save topics to Excel\n",
    "lda_topics_df.to_excel('lda_topics.xlsx', index=False)\n",
    "nmf_topics_df.to_excel('nmf_topics.xlsx', index=False)\n",
    "lda_tfidf_df.to_excel('lda_tfidf_topics.xlsx', index=False)\n",
    "nmf_bow_df.to_excel('nmf_bow_topics.xlsx', index=False)\n",
    "\n",
    "# Document-Topic Distribution\n",
    "lda_document_topics = pd.DataFrame(lda_output, columns=[f'LDA_Topic_{i+1}' for i in range(n_topics)])\n",
    "nmf_document_topics = pd.DataFrame(nmf_output, columns=[f'NMF_Topic_{i+1}' for i in range(n_topics)])\n",
    "lda_tfidf_doc = pd.DataFrame(lda_tfidf_output, columns=[f'LDA_TFIDF_Topic_{i+1}' for i in range(n_topics)])\n",
    "nmf_bow_doc = pd.DataFrame(nmf_bow_output, columns=[f'NMF_BoW_Topic_{i+1}' for i in range(n_topics)])\n",
    "\n",
    "# Add document-topic distributions to the original dataframe\n",
    "result_df = pd.concat([df, \n",
    "                       lda_document_topics, lda_tfidf_doc,\n",
    "                       nmf_document_topics, nmf_bow_doc], axis=1)\n",
    "result_df.to_excel('complaints_all_models.xlsx', index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total complaints processed: {len(df)}\")\n",
    "print(\"\\nLDA Topics (Bag-of-Words):\")\n",
    "for topic in lda_topics:\n",
    "    for topic_name, words in topic.items():\n",
    "        print(f\"{topic_name}: {', '.join(words)}\")\n",
    "print(\"\\nLDA Topics (TF-IDF):\")\n",
    "for topic in lda_tfidf_topics:\n",
    "    for topic_name, words in topic.items():\n",
    "        print(f\"{topic_name}: {', '.join(words)}\")\n",
    "print(\"\\nNMF Topics (TF-IDF):\")\n",
    "for topic in nmf_topics:\n",
    "    for topic_name, words in topic.items():\n",
    "        print(f\"{topic_name}: {', '.join(words)}\")\n",
    "print(\"\\nNMF Topics (BoW):\")\n",
    "for topic in nmf_bow_topics:\n",
    "    for topic_name, words in topic.items():\n",
    "        print(f\"{topic_name}: {', '.join(words)}\")\n",
    "print(\"\\nAll results have been saved to Excel files.\")\n",
    "\n",
    "# Compute dominant topic per document for each model \n",
    "def get_dominant_topic(doc_topic_matrix):\n",
    "    return doc_topic_matrix.idxmax(axis=1)  # Column with highest value\n",
    "\n",
    "# Function to get dominant topic for each document\n",
    "def get_dominant_topic(doc_topic_matrix):\n",
    "    return doc_topic_matrix.idxmax(axis=1)  # Column with highest probability\n",
    "\n",
    "# Compute dominant topics\n",
    "result_df['LDA_BoW_Dominant'] = get_dominant_topic(lda_document_topics)\n",
    "result_df['LDA_TFIDF_Dominant'] = get_dominant_topic(lda_tfidf_doc)\n",
    "result_df['NMF_TFIDF_Dominant'] = get_dominant_topic(nmf_document_topics)\n",
    "result_df['NMF_BoW_Dominant'] = get_dominant_topic(nmf_bow_doc)\n",
    "\n",
    "# Topic label mappings\n",
    "lda_bow_labels = {\n",
    "    'LDA_Topic_1': 'Credit & Reporting',\n",
    "    'LDA_Topic_2': 'Mortgage Servicing',\n",
    "    'LDA_Topic_3': 'Payments & Accounts',\n",
    "    'LDA_Topic_4': 'Insurance & Escrow',\n",
    "    'LDA_Topic_5': 'Servicing Errors'\n",
    "}\n",
    "\n",
    "lda_tfidf_labels = {\n",
    "    'LDA_TFIDF_Topic_1': 'Communication & Calls',\n",
    "    'LDA_TFIDF_Topic_2': 'Mortgage Foreclosure',\n",
    "    'LDA_TFIDF_Topic_3': 'Insurance & Escrow',\n",
    "    'LDA_TFIDF_Topic_4': 'Payment Issues',\n",
    "    'LDA_TFIDF_Topic_5': 'Credit Reporting'\n",
    "}\n",
    "\n",
    "nmf_tfidf_labels = {\n",
    "    'NMF_Topic_1': 'Communication & Loan Info',\n",
    "    'NMF_Topic_2': 'Mortgage Foreclosure',\n",
    "    'NMF_Topic_3': 'Insurance & Property',\n",
    "    'NMF_Topic_4': 'Payments & Fees',\n",
    "    'NMF_Topic_5': 'Credit Reporting'\n",
    "}\n",
    "\n",
    "nmf_bow_labels = {\n",
    "    'NMF_BoW_Topic_1': 'Credit Reporting',\n",
    "    'NMF_BoW_Topic_2': 'Mortgage & Servicing',\n",
    "    'NMF_BoW_Topic_3': 'Payment Issues',\n",
    "    'NMF_BoW_Topic_4': 'Insurance & Escrow',\n",
    "    'NMF_BoW_Topic_5': 'Servicing Errors'\n",
    "}\n",
    "\n",
    "# Map dominant topic number to label\n",
    "result_df['LDA_BoW_Label'] = result_df['LDA_BoW_Dominant'].map(lda_bow_labels)\n",
    "result_df['LDA_TFIDF_Label'] = result_df['LDA_TFIDF_Dominant'].map(lda_tfidf_labels)\n",
    "result_df['NMF_TFIDF_Label'] = result_df['NMF_TFIDF_Dominant'].map(nmf_tfidf_labels)\n",
    "result_df['NMF_BoW_Label'] = result_df['NMF_BoW_Dominant'].map(nmf_bow_labels)\n",
    "\n",
    "# Print topic prevalence for all models in desired format\n",
    "print(\"\\nTopic prevalence comparison (all models):\\n\")\n",
    "\n",
    "model_label_pairs = [\n",
    "    ('LDA_BoW_Label', lda_bow_labels),\n",
    "    ('LDA_TFIDF_Label', lda_tfidf_labels),\n",
    "    ('NMF_TFIDF_Label', nmf_tfidf_labels),\n",
    "    ('NMF_BoW_Label', nmf_bow_labels)\n",
    "]\n",
    "\n",
    "for col, label_map in model_label_pairs:\n",
    "    counts = result_df[col].value_counts()\n",
    "    print(f\"{col} Topic Prevalence:\")\n",
    "    # Print with Topic X: Label: N complaints\n",
    "    for i, (label, count) in enumerate(counts.items(), start=1):\n",
    "        # Find topic number from label_map (reverse lookup)\n",
    "        topic_num = [k.split('_')[-1] for k,v in label_map.items() if v == label][0]\n",
    "        print(f\"Topic {topic_num}: {label}: {count} complaints\")\n",
    "    print(\"\")  # Blank line for separation\n",
    "\n",
    "# Save results with labels to Excel\n",
    "result_df.to_excel('complaints_all_models_with_labels.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d8078-aa1a-44f8-b855-9d6aa0e3fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
